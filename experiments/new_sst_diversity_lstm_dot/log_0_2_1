latest model is experiments/new_sst_diversity_lstm_dot/sst/diversity_lstm+dot__diversity_weight_0_2/Fri_Oct_18_01:58:58_2019
old config {'model': {'encoder': {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'dot'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_2', 'diversity_weight': 0, 'entropy_weight': 2}}
new config {'model': {'encoder': {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'dot'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_2', 'diversity_weight': 0, 'entropy_weight': 2}}
encoder params {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128, 'pre_embed': None}
INFO - 2019-10-18 02:24:23,943 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2019-10-18 02:24:23,943 - type = customrnn
INFO - 2019-10-18 02:24:23,943 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 13826, 'embed_size': 300, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2019-10-18 02:24:23,953 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2019-10-18 02:24:24,125 - vocab_size = 13826
INFO - 2019-10-18 02:24:24,125 - embed_size = 300
INFO - 2019-10-18 02:24:24,125 - hidden_size = 128
INFO - 2019-10-18 02:24:24,125 - pre_embed = None
INFO - 2019-10-18 02:24:26,691 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'dot'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2019-10-18 02:24:26,692 - hidden_size = 256
INFO - 2019-10-18 02:24:26,692 - output_size = 1
INFO - 2019-10-18 02:24:26,692 - use_attention = True
INFO - 2019-10-18 02:24:26,692 - regularizer_attention = None
INFO - 2019-10-18 02:24:26,692 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x7fd179586128> and extras set()
INFO - 2019-10-18 02:24:26,693 - attention.type = dot
INFO - 2019-10-18 02:24:26,693 - type = dot
INFO - 2019-10-18 02:24:26,693 - instantiating class <class 'Transparency.model.modules.Attention.DotAttention'> from params <allennlp.common.params.Params object at 0x7fd179586128> and extras set()
INFO - 2019-10-18 02:24:26,693 - attention.hidden_size = 256
INFO - 2019-10-18 02:24:26,693 - hidden_size = 256
configuration {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_2', 'diversity_weight': 0, 'entropy_weight': 2}} {'model': {'encoder': {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'dot'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_2', 'diversity_weight': 0, 'entropy_weight': 2}}
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_2', 'diversity_weight': 0, 'entropy_weight': 2}}
/home/preksha/anaconda3/envs/attention/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/scratch2/preksha/Transparency/model/modules/bnlstm.py:67: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/scratch2/preksha/Transparency/model/modules/bnlstm.py:68: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
INFO - 2019-10-18 02:24:27,243 - Generating graph for experiments/new_sst_diversity_lstm_dot/sst/diversity_lstm+dot__diversity_weight_0_2/Fri_Oct_18_01:58:58_2019
INFO - 2019-10-18 02:24:27,244 - Average Length of test set 10
INFO - 2019-10-18 02:24:27,652 - Generating Gradients Graph ...
/home/preksha/anaconda3/envs/attention/lib/python3.6/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)
/home/preksha/anaconda3/envs/attention/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: 
The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.
  alternative="'density'", removal="3.1")
INFO - 2019-10-18 02:26:05,834 - Generating Permutations Graph ...
INFO - 2019-10-18 02:26:08,533 - Generating importance ranking Graph ...
{'accuracy': 0.8139130434782609, 'roc_auc': 0.9023720738910561, 'pr_auc': 0.9068110860307927, 'conicity_mean': '0.7234141', 'conicity_std': '0.17141998', 'entropy_mean': '0.15670806', 'entropy_std': '0.042008203'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.814    0.814      0.814      0.814         0.814
precision    0.815    0.813      0.814      0.814         0.814
recall       0.812    0.816      0.814      0.814         0.814
support    863.000  862.000   1725.000   1725.000      1725.000
pos tags ['NOUN', 'VERB', 'ADJ', 'DET', 'ADP', 'ADV', 'PRON', 'CONJ', 'PRT', 'NUM', 'X', '.']
words_positive ['<UNK>', 'the', 'and', 'a', 'of', 'is', 'to', 'it', 's', 'in']
words_negative ['<UNK>', 'the', 'a', 'and', 'of', 'to', 'it', 'is', 's', 'in']
xxe len shape 1725 (1, 8)
xxe len shape 1725 (1, 8)
xxe len shape 1725 (1, 8)
============================================================================================================================================================================================================================================================================================================
