old config {'model': {'encoder': {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'dot'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}}
new config {'model': {'encoder': {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'dot'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}}
encoder params {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128, 'pre_embed': None}
INFO - 2019-10-18 02:15:30,979 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2019-10-18 02:15:30,979 - type = customrnn
INFO - 2019-10-18 02:15:30,980 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 13826, 'embed_size': 300, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2019-10-18 02:15:30,989 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2019-10-18 02:15:31,154 - vocab_size = 13826
INFO - 2019-10-18 02:15:31,154 - embed_size = 300
INFO - 2019-10-18 02:15:31,154 - hidden_size = 128
INFO - 2019-10-18 02:15:31,154 - pre_embed = None
INFO - 2019-10-18 02:15:33,763 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'dot'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2019-10-18 02:15:33,763 - hidden_size = 256
INFO - 2019-10-18 02:15:33,763 - output_size = 1
INFO - 2019-10-18 02:15:33,763 - use_attention = True
INFO - 2019-10-18 02:15:33,763 - regularizer_attention = None
INFO - 2019-10-18 02:15:33,764 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x7f7d10b32128> and extras set()
INFO - 2019-10-18 02:15:33,764 - attention.type = dot
INFO - 2019-10-18 02:15:33,764 - type = dot
INFO - 2019-10-18 02:15:33,764 - instantiating class <class 'Transparency.model.modules.Attention.DotAttention'> from params <allennlp.common.params.Params object at 0x7f7d10b32128> and extras set()
INFO - 2019-10-18 02:15:33,764 - attention.hidden_size = 256
INFO - 2019-10-18 02:15:33,764 - hidden_size = 256
configuration {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}} {'model': {'encoder': {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'dot'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}}
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}}
{'accuracy': 0.8197101449275362, 'roc_auc': 0.8984683548727932, 'pr_auc': 0.9063415423759167, 'conicity_mean': '0.97075576', 'conicity_std': '0.0103093665', 'entropy_mean': '0.0038897994', 'entropy_std': '0.0014776393'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.824    0.815       0.82      0.820         0.820
precision    0.804    0.837       0.82      0.821         0.821
recall       0.846    0.794       0.82      0.820         0.820
support    863.000  862.000    1725.00   1725.000      1725.000
/home/preksha/anaconda3/envs/attention/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/scratch2/preksha/Transparency/model/modules/bnlstm.py:67: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/scratch2/preksha/Transparency/model/modules/bnlstm.py:68: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
Pos_attn
[('NOUN', [8889, 5.2707424, 0.0005929511099540822]), ('VERB', [5065, 2.258079, 0.00044582014846425184]), ('ADJ', [3928, 2.0706558, 0.0005271527043670841]), ('DET', [3784, 1.7344342, 0.00045836000185436225]), ('ADP', [3632, 1.4121268, 0.0003888014260892826]), ('ADV', [2104, 1.1799256, 0.0005608011221251107]), ('PRON', [1637, 0.673868, 0.0004116481371456046]), ('CONJ', [1322, 0.47826582, 0.00036177444926909706]), ('PRT', [796, 0.23991787, 0.00030140436474402347]), ('NUM', [248, 0.15002903, 0.0006049557799293149]), ('X', [17, 0.013319498, 0.0007834998781190199]), ('.', [2, 0.0028666153, 0.0014333076542243361])]
word_attn_positive
[('<UNK>', [851, 0.9659039474208839, 0.0011350222648894054]), ('the', [733, 0.29617528187372955, 0.0004040590475767115]), ('a', [601, 0.2897614536977926, 0.0004821322024921674]), ('and', [585, 0.19484030155217624, 0.0003330603445336346]), ('of', [495, 0.14250188472396985, 0.0002878825954019593]), ('it', [258, 0.13835781779198442, 0.0005362706115968389]), ('to', [313, 0.08546858759291354, 0.00027306258016905285]), ('is', [279, 0.07740216333695571, 0.0002774271087346083]), ('s', [268, 0.07421000356407603, 0.00027690299837341805]), ('an', [125, 0.07256439139382564, 0.0005805151311506051]), ('in', [219, 0.07068679922122101, 0.0003227707726996393]), ('this', [111, 0.07028332470508758, 0.0006331831054512395]), ('as', [132, 0.06049813844583696, 0.00045831923065027996]), ('that', [203, 0.054733407210733276, 0.00026962269561937576]), ('with', [144, 0.049564470289624296, 0.0003441977103446132]), ('film', [132, 0.042371363339043455, 0.0003209951768109353]), ('but', [120, 0.042065851055667736, 0.0003505487587972311]), ('for', [102, 0.038604349127126625, 0.00037847401105026103]), ('its', [113, 0.0320679937103705, 0.000283787554959031]), ('you', [83, 0.026771249278681353, 0.00032254517203230543])]
word_attn_negative
[('<UNK>', [923, 1.0382394452026347, 0.001124853136730915]), ('the', [773, 0.3737005876009789, 0.0004834418985782392]), ('a', [521, 0.21536003288383654, 0.0004133589882607227]), ('it', [256, 0.13028861669954495, 0.0005089399089825974]), ('and', [395, 0.1291977564433182, 0.00032708292770460305]), ('of', [416, 0.12456088419276057, 0.0002994252023864437]), ('to', [349, 0.10562632915389258, 0.00030265423826330255]), ('s', [245, 0.07614750440916396, 0.0003108061404455672]), ('is', [250, 0.07611253574214061, 0.00030445014296856245]), ('this', [131, 0.0658720017017913, 0.0005028397076472619]), ('in', [200, 0.06260881464913837, 0.00031304407324569185]), ('as', [155, 0.062455678071273724, 0.0004029398585243466]), ('too', [58, 0.06188020494300872, 0.0010669000852242883]), ('but', [119, 0.05349425975509803, 0.0004495315945806557]), ('that', [182, 0.05128519769641571, 0.00028178680052975665]), ('for', [125, 0.04935432600177592, 0.00039483460801420734]), ('movie', [134, 0.04402662449138006, 0.00032855689918940347]), ('t', [94, 0.04176763089344604, 0.0004443364988664472]), ('not', [69, 0.04125498606299516, 0.0005978983487390602]), ('with', [112, 0.03963352140635834, 0.0003538707268424852])]
xxe len shape 1725 (1, 8)
xxe len shape 1725 (1, 8)
xxe len shape 1725 (1, 8)
