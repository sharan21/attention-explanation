latest model is experiments/new_sst_diversity_lstm_dot/sst/diversity_lstm+dot__diversity_weight_0_10/Fri_Oct_18_02:10:04_2019
old config {'model': {'encoder': {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'dot'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}}
new config {'model': {'encoder': {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'dot'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}}
encoder params {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128, 'pre_embed': None}
INFO - 2019-10-18 02:23:59,497 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2019-10-18 02:23:59,497 - type = customrnn
INFO - 2019-10-18 02:23:59,497 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 13826, 'embed_size': 300, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2019-10-18 02:23:59,507 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2019-10-18 02:23:59,672 - vocab_size = 13826
INFO - 2019-10-18 02:23:59,673 - embed_size = 300
INFO - 2019-10-18 02:23:59,673 - hidden_size = 128
INFO - 2019-10-18 02:23:59,673 - pre_embed = None
INFO - 2019-10-18 02:24:02,222 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'dot'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2019-10-18 02:24:02,223 - hidden_size = 256
INFO - 2019-10-18 02:24:02,223 - output_size = 1
INFO - 2019-10-18 02:24:02,223 - use_attention = True
INFO - 2019-10-18 02:24:02,223 - regularizer_attention = None
INFO - 2019-10-18 02:24:02,223 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x7f4732e3a0f0> and extras set()
INFO - 2019-10-18 02:24:02,224 - attention.type = dot
INFO - 2019-10-18 02:24:02,224 - type = dot
INFO - 2019-10-18 02:24:02,224 - instantiating class <class 'Transparency.model.modules.Attention.DotAttention'> from params <allennlp.common.params.Params object at 0x7f4732e3a0f0> and extras set()
INFO - 2019-10-18 02:24:02,224 - attention.hidden_size = 256
INFO - 2019-10-18 02:24:02,224 - hidden_size = 256
configuration {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}} {'model': {'encoder': {'vocab_size': 13826, 'embed_size': 300, 'type': 'customrnn', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'dot'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}}
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.9135802469135803], 'basepath': 'experiments/new_sst_diversity_lstm_dot', 'exp_dirname': 'sst/diversity_lstm+dot__diversity_weight_0_10', 'diversity_weight': 0, 'entropy_weight': 10}}
/home/preksha/anaconda3/envs/attention/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/scratch2/preksha/Transparency/model/modules/bnlstm.py:67: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/scratch2/preksha/Transparency/model/modules/bnlstm.py:68: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
INFO - 2019-10-18 02:24:02,756 - Generating graph for experiments/new_sst_diversity_lstm_dot/sst/diversity_lstm+dot__diversity_weight_0_10/Fri_Oct_18_02:10:04_2019
INFO - 2019-10-18 02:24:02,757 - Average Length of test set 10
INFO - 2019-10-18 02:24:03,154 - Generating Gradients Graph ...
/home/preksha/anaconda3/envs/attention/lib/python3.6/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)
/home/preksha/anaconda3/envs/attention/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: 
The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.
  alternative="'density'", removal="3.1")
INFO - 2019-10-18 02:25:43,822 - Generating Permutations Graph ...
INFO - 2019-10-18 02:25:46,620 - Generating importance ranking Graph ...
{'accuracy': 0.8197101449275362, 'roc_auc': 0.8984683548727932, 'pr_auc': 0.9063415423759167, 'conicity_mean': '0.97075576', 'conicity_std': '0.0103093665', 'entropy_mean': '0.0038897994', 'entropy_std': '0.0014776393'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.824    0.815       0.82      0.820         0.820
precision    0.804    0.837       0.82      0.821         0.821
recall       0.846    0.794       0.82      0.820         0.820
support    863.000  862.000    1725.00   1725.000      1725.000
pos tags ['NOUN', 'VERB', 'ADJ', 'DET', 'ADP', 'ADV', 'PRON', 'CONJ', 'PRT', 'NUM', 'X', '.']
words_positive ['<UNK>', 'the', 'a', 'and', 'of', 'it', 'to', 'is', 's', 'an']
words_negative ['<UNK>', 'the', 'a', 'it', 'and', 'of', 'to', 's', 'is', 'this']
xxe len shape 1725 (1, 8)
xxe len shape 1725 (1, 8)
xxe len shape 1725 (1, 8)
============================================================================================================================================================================================================================================================================================================
